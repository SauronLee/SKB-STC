{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "61f69dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx\n",
    "import json\n",
    "import pickle\n",
    "import gensim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from utils import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7e17b3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = 'Ohsumed'\n",
    "STC_Benchmark_path = \"../benchmark\"\n",
    "NumOfTrainTextPerClass = 2\n",
    "TOPK = 10\n",
    "SIM_MIN = 0.5\n",
    "g = networkx.Graph()\n",
    "\n",
    "train_: dict = np.load(\"{}/{}/train.npy\".format(STC_Benchmark_path,DATASETS), allow_pickle=True).tolist()\n",
    "test_: dict = np.load(\"{}/{}/test.npy\".format(STC_Benchmark_path,DATASETS), allow_pickle=True).tolist()\n",
    "train_tagme: list = np.load(\"{}/{}/train_tagme.npy\".format(STC_Benchmark_path,DATASETS), allow_pickle=True).tolist()\n",
    "test_tagme: list = np.load(\"{}/{}/test_tagme.npy\".format(STC_Benchmark_path,DATASETS), allow_pickle=True).tolist()\n",
    "all_: dict = np.load(\"{}/{}/all.npy\".format(STC_Benchmark_path,DATASETS), allow_pickle=True).tolist()\n",
    "all_tagme: list = np.load(\"{}/{}/all_tagme.npy\".format(STC_Benchmark_path,DATASETS), allow_pickle=True).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d705e0",
   "metadata": {},
   "source": [
    "### load text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8ab1c458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadText(data_dict, trainNumPerClass=20):\n",
    "    datapath = \"{}/{}/\".format(STC_Benchmark_path,DATASETS)\n",
    "    X = []\n",
    "    Y = []\n",
    "    catemap = dict()\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    for index, (key, value) in enumerate(data_dict.items()):\n",
    "        X.append([index])\n",
    "        Y.append(value[0])\n",
    "    cateset = list(set(Y))\n",
    "    catemap = dict()\n",
    "    for i in range(len(cateset)):\n",
    "        catemap[cateset[i]] = i\n",
    "    Y = [catemap[i] for i in Y]\n",
    "    X = np.array(X)\n",
    "    trainNum = trainNumPerClass*len(catemap)\n",
    "    ind_train, ind_test = train_test_split(X,train_size=trainNum, random_state=1, )\n",
    "    ind_vali, ind_test = train_test_split(ind_test,train_size=trainNum/(len(X)-trainNum), random_state=1, )\n",
    "    train = sum(ind_train.tolist(), [])\n",
    "    vali = sum(ind_vali.tolist(), [])\n",
    "    test = sum(ind_test.tolist(), [])\n",
    "    alltext = set(train + vali + test)\n",
    "    print( \"train: {}\\nvali: {}\\ntest: {}\\nAllTexts: {}\".format( len(train), len(vali), len(test), len(alltext)) )\n",
    "    with open(datapath+'train.list', 'w') as f:\n",
    "        f.write( '\\n'.join(map(str, train)) )\n",
    "    with open(datapath+'vali.list', 'w') as f:\n",
    "        f.write( '\\n'.join(map(str, vali)) )\n",
    "    with open(datapath+'test.list', 'w') as f:\n",
    "        f.write( '\\n'.join(map(str, test)) )\n",
    "    return train, vali, test, alltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "803023ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 460\n",
      "vali: 459\n",
      "test: 6479\n",
      "AllTexts: 7398\n"
     ]
    }
   ],
   "source": [
    "train, vali, test, alltext = loadText(dict(train_,**test_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b797aa6a",
   "metadata": {},
   "source": [
    "### load entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1e1a0e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTagMeEntity(tagme_list):\n",
    "    entitySet = set()\n",
    "    rho = 0.1\n",
    "    noEntity = set()\n",
    "    for line in tqdm(tagme_list, desc=\"tagme_list: \"):\n",
    "        ind, entityList = line.strip('\\n').split('\\t')\n",
    "        if int(ind) not in alltext:\n",
    "            continue\n",
    "        entityList = json.loads(entityList)\n",
    "        entities = [(d['title'].replace(\" \", '_'), d['rho'], d['link_probability'])\\\n",
    "                        for d in entityList if 'title' in d and float(d['rho']) > rho]\n",
    "        \n",
    "        entitySet.update([d['title'].replace(\" \", '_')\\\n",
    "                        for d in entityList if 'title' in d and float(d['rho']) > rho])\n",
    "        g.add_edges_from([(ind, e[0], {'rho': e[1], 'link_probability': e[2]}) for e in entities])\n",
    "        if len(entities) == 0:\n",
    "            noEntity.add(ind)\n",
    "            g.add_node(ind)\n",
    "    return entitySet, noEntity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "660d3375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tagme_list: 100%|██████████| 7400/7400 [00:00<00:00, 30802.64it/s]\n"
     ]
    }
   ],
   "source": [
    "entitySet, noEntity = loadTagMeEntity(train_tagme+test_tagme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8b492303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34912"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c6efff",
   "metadata": {},
   "source": [
    "### load labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "fdb2702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadLabel(data_dict):\n",
    "    for index, (key, value) in enumerate(data_dict.items()):\n",
    "        if index not in alltext:\n",
    "            continue\n",
    "        ind_str = str(index)\n",
    "        if ind_str not in g.nodes():\n",
    "            g.add_node(ind_str)\n",
    "        g.nodes[ind_str]['type'] = value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "67ebcf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadLabel(dict(train_,**test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "40e2a14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#g._node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75e7f4e",
   "metadata": {},
   "source": [
    "### load similarities between entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e20f798f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/hgat/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400001 \n",
      " 300\n"
     ]
    }
   ],
   "source": [
    "glove_input_file = \"/Users/sauron/Desktop/Code/SKB-STC/data/embeddings/glove.6B.300d.wiki.txt\"\n",
    "word2vec_output_file = './data/glove.6B.300d.word2vec.txt'\n",
    "(count, dimensions) = glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "print(count, '\\n', dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "4238a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d0aa00",
   "metadata": {},
   "source": [
    "### topK + 阈值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "341ccab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cardiac_tamponade'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "el[1].strip(')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b5e9b480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entToEnt(entitySet):\n",
    "    sim_min = SIM_MIN\n",
    "    topK = TOPK\n",
    "    el = list(entitySet)\n",
    "    entity_edge = []\n",
    "    cnt_no = 0\n",
    "    cnt_yes = 0\n",
    "    cnt = 0\n",
    "    for i in tqdm(range(len(el)), desc=\"entity to entity: \"):\n",
    "        simList = []\n",
    "        topKleft = topK\n",
    "        for j in range(len(el)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            cnt += 1\n",
    "            try:\n",
    "                sim = model.similarity(el[i].lower().strip(')'), el[j].lower().strip(')'))\n",
    "                cnt_yes += 1\n",
    "                if sim >= sim_min:\n",
    "                    entity_edge.append( (el[i], el[j], {'sim': sim}) )\n",
    "                    topKleft -= 1\n",
    "                else:\n",
    "                    simList.append( (sim, el[j]) )\n",
    "            except Exception as e:\n",
    "                cnt_no += 1\n",
    "        simList = sorted(simList, key=(lambda x: x[0]), reverse=True)\n",
    "        for i in range(min(max(topKleft, 0), len(simList))):\n",
    "            entity_edge.append( (el[i], simList[i][1], {'sim': simList[i][0]}) )\n",
    "    print(cnt_yes, cnt_no)\n",
    "    return entity_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "e560b083",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entity to entity: 100%|██████████| 7666/7666 [14:22<00:00,  8.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5633502 53126388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "entity_edge = entToEnt(entitySet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "22405b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.add_edges_from(entity_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4895bd",
   "metadata": {},
   "source": [
    "### save the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "0b72dd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/model_network.pkl', 'wb') as f:\n",
    "    pickle.dump(g, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deea1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hgat",
   "language": "python",
   "name": "hgat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
