{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0f92e3f",
   "metadata": {},
   "source": [
    "# Text Classification  SKB Graph on Ohsumed via Attention\n",
    "* By Xiaoran Li (Shizuoka Institute of Science and Technology) for JSAI2022\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "346fc0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/adaptsystemlab2019/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/adaptsystemlab2019/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/adaptsystemlab2019/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import tqdm\n",
    "import sys\n",
    "import json\n",
    "import spacy\n",
    "from scipy import linalg, mat, dot, stats\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict\n",
    "from scipy import linalg, mat, dot, stats\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "dataset = \"ohsumed\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "234b4bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = re.sub(r\"patients \", \"\", string)\n",
    "    return string.strip().lower()\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "def lemmatization(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tagged_sent = pos_tag(tokens)\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemmas_sent = []\n",
    "    for tag in tagged_sent:\n",
    "        wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "        lemmas_sent.append((wnl.lemmatize(tag[0], pos = wordnet_pos),wordnet_pos))\n",
    "    return lemmas_sent\n",
    "\n",
    "def lemmatization_original(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tagged_sent = pos_tag(tokens)\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemmas_sent = []\n",
    "    for tag in tagged_sent:\n",
    "        wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "        lemmas_sent.append(wnl.lemmatize(tag[0], pos = wordnet_pos))\n",
    "    return lemmas_sent\n",
    "\n",
    "def getPos(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tagged_sent = pos_tag(tokens)\n",
    "    sent = []\n",
    "    for tag in tagged_sent:\n",
    "        wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "        sent.append((tag[0], wordnet_pos))\n",
    "    return sent\n",
    "\n",
    "def countMinMaxAver(lines):\n",
    "    min_len = 10000\n",
    "    aver_len = 0\n",
    "    max_len = 0\n",
    "    for temp in lines:\n",
    "        aver_len = aver_len + len(temp)\n",
    "        if len(temp) < min_len:\n",
    "            min_len = len(temp)\n",
    "        if len(temp) > max_len:\n",
    "            max_len = len(temp)\n",
    "    aver_len = 1.0 * aver_len / len(lines)\n",
    "    print('min_len : ' + str(min_len))\n",
    "    print('max_len : ' + str(max_len))\n",
    "    print('average_len : ' + str(aver_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f03be18",
   "metadata": {},
   "source": [
    "## remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "71884f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _getTextFile(langual):\n",
    "    file_list = glob.glob(f'../data/stopwords/stopwords/*_{langual}.txt')\n",
    "    files = \",\".join(file_list)\n",
    "    return files\n",
    "def cleanText(english_txt):\n",
    "    try:\n",
    "        word_tokens = english_txt.split()\n",
    "        filtered_word = [w for w in word_tokens if w not in stop_words and not w.isdigit()]\n",
    "        filtered_word = [w + \" \" for w in filtered_word]\n",
    "        return \"\".join(filtered_word)\n",
    "    except:\n",
    "        return np.nan\n",
    "def cleanNonEnglish(txt):\n",
    "    txt = clean_str(txt)\n",
    "    txt = re.sub(r'\\W+', ' ', txt)\n",
    "    txt = txt.lower()\n",
    "    txt = txt.replace(\"[^a-zA-Z]\", \" \")\n",
    "    word_tokens = txt.split()\n",
    "    filtered_word = [w for w in word_tokens if all(ord(c) < 128 for c in w)]\n",
    "    filtered_word = [w + \" \" for w in filtered_word]\n",
    "    return \"\".join(filtered_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "c2e70931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_len : 28\n",
      "max_len : 596\n",
      "average_len : 186.03851351351352\n"
     ]
    }
   ],
   "source": [
    "stop_words = set()\n",
    "for file in _getTextFile(\"en\").split(\",\"):\n",
    "    for word in open(file):\n",
    "        stop_words.add(word.strip())\n",
    "doc_content_list = []\n",
    "f = open('../data/corpus/' + dataset + '.txt', 'rb')\n",
    "for line in f.readlines():\n",
    "    doc_content_list.append(line.strip().decode('latin1'))\n",
    "f.close()\n",
    "countMinMaxAver([item.split() for item in doc_content_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9b3a7b",
   "metadata": {},
   "source": [
    "For STC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "78fa8ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_len : 1\n",
      "max_len : 36\n",
      "average_len : 11.927432432432433\n"
     ]
    }
   ],
   "source": [
    "stop_words = set()\n",
    "for file in _getTextFile(\"en\").split(\",\"):\n",
    "    for word in open(file):\n",
    "        stop_words.add(word.strip())\n",
    "        \n",
    "doc_content_list = np.load(\"Ohsumed_stc_list.npy\", allow_pickle=True).tolist()\n",
    "\n",
    "countMinMaxAver([item.split() for item in doc_content_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f512f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9ca7293",
   "metadata": {},
   "source": [
    "## lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "1d09258d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Infection in total joint replacement.  Although a small number of infections in total joint replacements are blood borne from distant sources, most infections appear to have been derived at operation.  Strenuous attempts to reduce this risk by cleaning the air in the wound environment, coupled with prophylactic antibiotics, have reduced infection rates by an order of magnitude in a decade.  During that time the potential for exchange arthroplasty in established infection has been shown, and the results are encouraging.  Rigorous infection control is the key to containing this difficult and expensive problem.'"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_content_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "c53bbd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_content_list = [cleanText(cleanNonEnglish(sentence).strip()).strip() for sentence in doc_content_list]\n",
    "doc_content_lemmatization_list = [\" \".join(lemmatization_original(sentence)) for sentence in doc_content_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "308cde2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'infection total joint replacement small number infections total joint replacements blood borne distant sources infections derived operation strenuous attempts reduce risk cleaning air wound environment coupled prophylactic antibiotics reduced infection rates order magnitude decade time potential exchange arthroplasty established infection encouraging rigorous infection control key difficult expensive problem'"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_content_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "e5f4eb2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_content_list[0].split())==len(doc_content_lemmatization_list[0].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496d9ad8",
   "metadata": {},
   "source": [
    "## Analysis for OOV on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "8499f8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordFreq(docs):\n",
    "    word_freq = {}\n",
    "    for doc in docs:\n",
    "        for word in doc.split():\n",
    "            if word in word_freq:\n",
    "                word_freq[word] += 1\n",
    "            else:\n",
    "                word_freq[word] = 1\n",
    "    return word_freq\n",
    "def evaluationFrequncy(docs,save_name,limit_num):\n",
    "    '''evaluation frequncy for document lexions'''\n",
    "    word_freq = wordFreq(docs)\n",
    "    print(\"=======analysis start=======\")\n",
    "    print(\"#all word size: \", len(word_freq))\n",
    "    limit_word_freq_len = len([v for k,v in word_freq.items() if v < limit_num])\n",
    "    word_freq_sorted = sorted(word_freq.items(), key = lambda kv:(kv[1], kv[0]))\n",
    "    print(\"#frequncy < \"+ str(limit_num) +\": \", limit_word_freq_len)\n",
    "    print(\"#frequncy mean: \", np.mean(list(word_freq.values())))\n",
    "    print(\"#frequncy standard deviation: \", np.std(list(word_freq.values())))\n",
    "    print(\"#frequncy std/mean: \", np.std(list(word_freq.values()))/np.mean(list(word_freq.values())))\n",
    "    #axes = sns.scatterplot(data=list(word_freq.values())).set_title(save_name)\n",
    "    #axes.figure.set_size_inches(18,4)\n",
    "    #fig = axes.get_figure()\n",
    "    #fig.savefig(\"../data/images/\"+save_name+\".png\", dpi = 400)\n",
    "    return word_freq, word_freq_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "2b62251b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======analysis start=======\n",
      "#all word size:  31963\n",
      "#frequncy < 5:  18773\n",
      "#frequncy mean:  22.502362106185277\n",
      "#frequncy standard deviation:  96.59746181770852\n",
      "#frequncy std/mean:  4.29276985953206\n",
      "=======analysis start=======\n",
      "#all word size:  28897\n",
      "#frequncy < 5:  17403\n",
      "#frequncy mean:  24.8898847631242\n",
      "#frequncy standard deviation:  122.58557512115955\n",
      "#frequncy std/mean:  4.925116218407614\n"
     ]
    }
   ],
   "source": [
    "raw_word_freq, raw_word_freq_sorted = evaluationFrequncy(doc_content_list,\"raw_ohsumed\",5)\n",
    "lemm_word_freq, lemm_word_freq_sorted = evaluationFrequncy(doc_content_lemmatization_list,\"lemmatization_ohsumed\",5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "a0c504ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_content_tuple = (raw_word_freq,raw_word_freq_sorted,doc_content_list)\n",
    "doc_content_lemmatization_tuple = (lemm_word_freq,lemm_word_freq_sorted,doc_content_lemmatization_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "2cdf6bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../data/corpus/\" + dataset+\".clean\", doc_content_tuple)\n",
    "np.save(\"../data/corpus/\" + dataset+\".clean.lemmatization\", doc_content_lemmatization_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1500767",
   "metadata": {},
   "source": [
    "## Get the DictSKB and own SKB-DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "07a32e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_content_lemmatization_tuple: tuple = np.load(\"../data/corpus/\" + dataset+\".clean.lemmatization.npy\",\\\n",
    "                                                 allow_pickle=True).tolist()\n",
    "doc_content_tuple: tuple = np.load(\"../data/corpus/\" + dataset+\".clean.npy\", allow_pickle=True).tolist()\n",
    "dictskb = np.load(\"../sememe_dataset/DictSKB_dict.npy\", allow_pickle=True).tolist()\n",
    "dictskb_cdv = np.load(\"../sememe_dataset/DictSKB_sememes.npy\", allow_pickle=True).tolist()\n",
    "skb_da = np.load(\"../sememe_dataset/skb_ad_dict.npy\", allow_pickle=True).tolist()\n",
    "networkskb = np.load(\"../sememe_dataset/sememe_network_dict_en_wordnet_5000.npy\", allow_pickle=True).tolist()\n",
    "networkskb_cdv = np.load(\"../sememe_dataset/sememe_network_cdv_en_wordnet_5000.npy\", allow_pickle=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "8e9b8793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanSKB(skb):\n",
    "    clean_skb = {}\n",
    "    for word, items in tqdm.tqdm(skb.items()):\n",
    "        if word not in clean_skb.keys():\n",
    "            clean_skb[word] = []\n",
    "        for (pos, sememe_set) in items:\n",
    "            if word in clean_skb.keys() and len(sememe_set) != 0: \n",
    "                clean_skb[word].append((pos, sememe_set))\n",
    "    return clean_skb\n",
    "\n",
    "def removeWikiSenseOnSKBDA(skb_da_dict):\n",
    "    '''uniform for SKB-DA sense'''\n",
    "    skb_da_pure_dict = {}\n",
    "    skb_da_cdv_set = set()\n",
    "    for word, sense in tqdm.tqdm(skb_da_dict.items()):\n",
    "        for (pos, sememe_set) in sense:\n",
    "            if \" (\" in word:\n",
    "                if len(word.split(\" (\")) == 3:\n",
    "                    word, sense1,sense2 = word.split(\" (\")\n",
    "                    sense1 = sense1.replace(\")\",\"\")\n",
    "                    sense2 = sense2.replace(\")\",\"\")\n",
    "                    if word not in skb_da_pure_dict.keys():\n",
    "                        skb_da_pure_dict[word] = []\n",
    "                    #sememe_set.add(sense1)\n",
    "                    #sememe_set.add(sense2)\n",
    "                    sememe_set.discard(word)\n",
    "                    skb_da_pure_dict[word].append((sense1+\" - \"+sense2,sememe_set))\n",
    "                    skb_da_cdv_set =  skb_da_cdv_set | sememe_set\n",
    "                    continue\n",
    "                word, sense = word.split(\" (\")\n",
    "                sense = sense.replace(\")\",\"\")\n",
    "                if word not in skb_da_pure_dict.keys():\n",
    "                    skb_da_pure_dict[word] = []\n",
    "                #sememe_set.add(sense)\n",
    "                sememe_set.discard(word)\n",
    "                skb_da_pure_dict[word].append((sense,sememe_set))\n",
    "                skb_da_cdv_set =  skb_da_cdv_set | sememe_set\n",
    "            else:\n",
    "                if word not in skb_da_pure_dict.keys():\n",
    "                    skb_da_pure_dict[word] = []\n",
    "                sememe_set.discard(word)\n",
    "                skb_da_pure_dict[word].append((pos,sememe_set))\n",
    "                skb_da_cdv_set =  skb_da_cdv_set | sememe_set\n",
    "    print(\"#all lexicon of SKB-DA: {}; #CDV of SKB-DA: {}\".format(len(skb_da_pure_dict),len(skb_da_cdv_set)))\n",
    "    return cleanSKB(skb_da_pure_dict),skb_da_cdv_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "80362bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 910369/910369 [01:29<00:00, 10128.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#all lexicon of SKB-DA: 800795; #CDV of SKB-DA: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 800795/800795 [00:00<00:00, 841267.38it/s]\n"
     ]
    }
   ],
   "source": [
    "skb_da_pure,skb_da_pure_cdv_set = removeWikiSenseOnSKBDA(skb_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e0c01e",
   "metadata": {},
   "source": [
    "### for sememes of each word are limit to 4 via word embedding similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "a0f7cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos(vec1,vec2):\n",
    "    return vec1.dot(vec2)/(linalg.norm(vec1)*linalg.norm(vec2))\n",
    "    \n",
    "def catSememe(word, sememe_set, embedding_dict, upper_max):\n",
    "    order_embedding_list = []\n",
    "    if word not in embedding_dict.keys():\n",
    "        word = '<unk>'\n",
    "    for sememe in sememe_set:\n",
    "        if sememe in embedding_dict:\n",
    "            order_embedding_list.append(cos(embedding_dict[word],embedding_dict[sememe]))\n",
    "        else:\n",
    "            order_embedding_list.append(cos(embedding_dict[word],embedding_dict['<unk>']))\n",
    "    sememe_set = list(sememe_set)\n",
    "    return set(sememe_set[order_embedding_list.index(v)] for v in sorted(order_embedding_list)[-upper_max:])\n",
    "    \n",
    "def upperMaxSKB(skb_dict, embedding_dict, upper_max):\n",
    "    limit_skb = {}\n",
    "    for word, items in tqdm.tqdm(skb_dict.items()):\n",
    "        if word not in limit_skb.keys():\n",
    "            limit_skb[word] = []\n",
    "        for (pos, sememe_set) in items:\n",
    "            if len(sememe_set) > upper_max:\n",
    "                a = catSememe(word, sememe_set,embedding_dict,upper_max)\n",
    "                limit_skb[word].append((pos, a))\n",
    "                #print(word,a)\n",
    "            else:\n",
    "                limit_skb[word].append((pos, sememe_set))\n",
    "    return limit_skb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "1722ef50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 800795/800795 [00:36<00:00, 21965.27it/s]\n"
     ]
    }
   ],
   "source": [
    "skb_da_upper_max = upperMaxSKB(skb_da_pure,glove_840B_300d_common_crawl,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbd21ef",
   "metadata": {},
   "source": [
    "### clean the NetWordSKB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "b66032eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _networkskbForm(networkskb):\n",
    "    '''uniform for NetWorkSKB'''\n",
    "    networkskb_form = {}\n",
    "    for wn_k,wn_v in networkskb.items():\n",
    "        word = wn_k.split(\".\")[0]\n",
    "        if word not in networkskb_form.keys():\n",
    "            networkskb_form[word] = []\n",
    "        networkskb_form[word].append((wn_k.split(\".\")[1],wn_v))\n",
    "    return networkskb_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "666a64d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "networkskb = _networkskbForm(networkskb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfeb629",
   "metadata": {},
   "source": [
    "### How many words of  doc_content are in the lexicons of SKB-DA\n",
    ">* doc_content_lemmatization SKB-DA\n",
    ">* doc_content DictSKB\n",
    ">* doc_content_lemmatization DictSKB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "0f81f749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkExistFreq(doc_tuple,skb_dict,label_name_str):\n",
    "    '''How many words of doc_content are in the lexicons of SKB-DA'''\n",
    "    exist_freq = wordFreq([\" \".join([word for word in sentence.split() if word in skb_dict.keys()])\\\n",
    "                          for sentence in doc_tuple[2]])\n",
    "    #exist_freq_df = pd.DataFrame({label_name_str: list(exist_freq.values())})\n",
    "    #axes = sns.scatterplot(data = exist_freq_df)\n",
    "    #axes.figure.set_size_inches(18,4)\n",
    "    #fig = axes.get_figure()\n",
    "    #fig.savefig(\"../data/images/\"+doc_content_tuple+\".exist_freq.png\", dpi = 400)\n",
    "    print(len(exist_freq)/len(doc_tuple[1]))\n",
    "    return len(exist_freq)/len(doc_tuple[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "4fc01418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7011095700416089\n",
      "0.6446929389485075\n",
      "0.6764414503665543\n",
      "0.6071782847867255\n",
      "0.5761838716068952\n",
      "0.5080710614122103\n",
      "0.40855954032098274\n",
      "0.3437640905401749\n"
     ]
    }
   ],
   "source": [
    "_ = checkExistFreq(doc_content_lemmatization_tuple,skb_da_pure,\"lemmatization on DictSKB(Pure)\")\n",
    "_ = checkExistFreq(doc_content_tuple,skb_da_pure,\"raw on DictSKB(Pure)\")\n",
    "_ = checkExistFreq(doc_content_lemmatization_tuple,skb_da,\"lemmatization on SKB_DA\")\n",
    "_ = checkExistFreq(doc_content_tuple,skb_da,\"raw on SKB_DA\")\n",
    "_ = checkExistFreq(doc_content_lemmatization_tuple,networkskb,\"lemmatization on NetWorkSKB\")\n",
    "_ = checkExistFreq(doc_content_tuple,networkskb,\"raw on NetWorkSKB\")\n",
    "_ = checkExistFreq(doc_content_lemmatization_tuple,dictskb,\"lemmatization on DictSKB\")\n",
    "_ = checkExistFreq(doc_content_tuple,dictskb,\"raw on DictSKB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dcbb22",
   "metadata": {},
   "source": [
    "## the evaluation for embeddings of lexicon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "918a64d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    '''Loading Glove Model'''\n",
    "    f = open(gloveFile,'r', encoding='utf8')\n",
    "    model = {}\n",
    "    for line in tqdm.tqdm(f):\n",
    "        splitLine = line.split(' ')\n",
    "        word = splitLine[0]\n",
    "        embedding = np.asarray(splitLine[1:], dtype='float32')\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a3fd4405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400001it [00:13, 29020.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. 400001  words loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "glove_6B_300d_wiki = loadGloveModel(\"../data/embeddings/glove.6B.300d.wiki.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bfdb8be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1193517it [00:29, 40860.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. 1193515  words loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "glove_twitter_27B_200d_txt = loadGloveModel(\"../data/embeddings/glove.twitter.27B.200d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1f3ef66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196018it [01:10, 31079.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. 2196017  words loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "glove_840B_300d_common_crawl = loadGloveModel(\"../data/embeddings/glove.840B.300d.common_crawl.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d6bda45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkEmbeddingsLexicon(doc_content_tuple, skb_dict, skb_cdv, embedding_dict):\n",
    "    '''check the number of embedding in doc and skb'''\n",
    "    embeddings_in_doc = len(set(doc_content_tuple[0].keys()) & set(embedding_dict.keys()))/\\\n",
    "                        len(set(doc_content_tuple[0].keys()))\n",
    "    embeddings_in_skb_key = len(set(skb_dict.keys()) & set(embedding_dict.keys()))/\\\n",
    "                            len(set(skb_dict.keys()))\n",
    "    embeddings_in_skb_sememe = len(skb_cdv & set(embedding_dict.keys()))/\\\n",
    "                               len(skb_cdv)\n",
    "    print(\"The Embedding Lexion in doc: {}; in SKB keys: {}; in SKB Sememes: {}\"\\\n",
    "          .format(embeddings_in_doc,embeddings_in_skb_key,embeddings_in_skb_sememe))\n",
    "    return embeddings_in_doc,embeddings_in_skb_key,embeddings_in_skb_sememe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "90a278b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Embedding Lexion in doc: 0.4780427033948161; in SKB keys: 0.12749455228866313; in SKB Sememes: 0.8496\n",
      "The Embedding Lexion in doc: 0.5076181835247005; in SKB keys: 0.12749455228866313; in SKB Sememes: 0.8496\n"
     ]
    }
   ],
   "source": [
    "_,_,_ = checkEmbeddingsLexicon(doc_content_lemmatization_tuple,skb_da_pure,skb_da_pure_cdv_set,\\\n",
    "                               glove_twitter_27B_200d_txt)\n",
    "_,_,_ = checkEmbeddingsLexicon(doc_content_tuple,skb_da_pure,skb_da_pure_cdv_set,glove_twitter_27B_200d_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7f01cc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Embedding Lexion in doc: 0.6542547669308233; in SKB keys: 0.18834782934458882; in SKB Sememes: 0.8918\n",
      "The Embedding Lexion in doc: 0.6828207615054908; in SKB keys: 0.18834782934458882; in SKB Sememes: 0.8918\n"
     ]
    }
   ],
   "source": [
    "_,_,_ = checkEmbeddingsLexicon(doc_content_lemmatization_tuple,skb_da_pure,skb_da_pure_cdv_set,glove_6B_300d_wiki)\n",
    "_,_,_ = checkEmbeddingsLexicon(doc_content_tuple,skb_da_pure,skb_da_pure_cdv_set,glove_6B_300d_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "23a20dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Embedding Lexion in doc: 0.7939578502958785; in SKB keys: 0.15564782497393215; in SKB Sememes: 0.999\n",
      "The Embedding Lexion in doc: 0.812501955385915; in SKB keys: 0.15564782497393215; in SKB Sememes: 0.999\n"
     ]
    }
   ],
   "source": [
    "_,_,_ = checkEmbeddingsLexicon(doc_content_lemmatization_tuple,skb_da_pure,skb_da_pure_cdv_set,\\\n",
    "                               glove_840B_300d_common_crawl)\n",
    "_,_,_ = checkEmbeddingsLexicon(doc_content_tuple,skb_da_pure,skb_da_pure_cdv_set,glove_840B_300d_common_crawl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e08f14d",
   "metadata": {},
   "source": [
    "## Replace the word to sememe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "915ae72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CosineSimilarity(x1, x2):\n",
    "    x2 = x2.t()\n",
    "    x = x1.mm(x2)\n",
    "    x1_frobenius = x1.norm(dim=1).unsqueeze(0).t()\n",
    "    x2_frobenins = x2.norm(dim=0).unsqueeze(0)\n",
    "    x_frobenins = x1_frobenius.mm(x2_frobenins)\n",
    "    final = x.mul(1/x_frobenins)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "ab23d073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def localAttention(sentence_str, word_str, sememe_list, embedding_dict):\n",
    "    '''\n",
    "        input: sentence_str, word_str, embedding_dict\n",
    "        oytput: word embedding\n",
    "    '''\n",
    "    #print(sentence_str, word_str)\n",
    "    context_embedding_list = []\n",
    "    sentence_list = sentence_str.split()\n",
    "    if sentence_str != word_str:\n",
    "        sentence_list.remove(word_str)\n",
    "    for word in sentence_list:\n",
    "        if word in embedding_dict.keys():\n",
    "            context_embedding_list.append(embedding_dict[word])\n",
    "        else:\n",
    "            context_embedding_list.append(embedding_dict['<unk>'])\n",
    "\n",
    "    context_embedding_list = torch.Tensor(context_embedding_list).to(device)\n",
    "    word_embedding = torch.from_numpy(embedding_dict[word_str]).float().unsqueeze(0).to(device)\n",
    "    #print(context_embedding_list.size(),word_embedding.size())\n",
    "    cos_value_w = CosineSimilarity(context_embedding_list, word_embedding).to(device)\n",
    "    softmax_nn = nn.Softmax(dim=0)\n",
    "    softmax_weight_w = softmax_nn(cos_value_w) * 4\n",
    "    '''\n",
    "        get local word embedding\n",
    "    '''\n",
    "    local_word_embedding = softmax_weight_w.t().mm(context_embedding_list)\n",
    "    \n",
    "    \n",
    "    sememes_embedding_list = []\n",
    "    for sememe in sememe_list:\n",
    "        if sememe in embedding_dict.keys():\n",
    "            sememes_embedding_list.append(embedding_dict[sememe])\n",
    "        else:\n",
    "            sememes_embedding_list.append(embedding_dict['<unk>'])\n",
    "            \n",
    "    sememes_embedding_list = torch.Tensor(sememes_embedding_list).to(device)\n",
    "    cos_value_s = CosineSimilarity(sememes_embedding_list, local_word_embedding).to(device)\n",
    "    softmax_weight_s = softmax_nn(cos_value_s) * 2\n",
    "    local_sememe_embedding = softmax_weight_s.t().mm(sememes_embedding_list)\n",
    "    cos_value = CosineSimilarity(local_sememe_embedding,local_word_embedding)\n",
    "    \n",
    "    return cos_value.to('cpu').squeeze(0).numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "bc141d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceWord2Sememe(embedding_dict, skb_dict, docs_tuple,threshold):\n",
    "    '''\n",
    "        input: \n",
    "        process: if the word of sentence in skb && else if the freqency of word less then threshold:\n",
    "                    replace the word to sememe:\n",
    "                        if the sense of word only once:\n",
    "                            straightforward replace else more thinking... of (sense dismatching- now leave aside)\n",
    "                        else:\n",
    "                            search the sentence embedding of docs by look-up embedding dictionary\n",
    "                            for building the word embedding with weighted sum of sentence:\n",
    "                                senses cosin = list\n",
    "                                for index, sense the enumerate(senses):\n",
    "                                    word cosin = list\n",
    "                                    for sememe in sense:\n",
    "                                        compare both that the embedding of the word and the sememe of the sense\n",
    "                                        append the cosin value to word cosin list\n",
    "                                    keep minimum of senses cosin to append the senses cosin list\n",
    "                                get the index of minimum value for sense senses list\n",
    "                                get the word via index with this word senses of SKB-DA\n",
    "                            replace  \n",
    "        return: docs list replaced with sememe\n",
    "    '''\n",
    "    sememe_docs_list = []\n",
    "\n",
    "    '''\n",
    "        threshold = np.mean(list(docs_tuple[0].values())) + np.std(list(word_freq.values())) /\\\n",
    "                np.mean(list(word_freq.values()))\n",
    "    '''\n",
    "    threshold = threshold\n",
    "    for sentence in tqdm.tqdm(docs_tuple[2]):\n",
    "        sentence_replace = []\n",
    "        for word in sentence.split():\n",
    "            #print(\"####\",word)\n",
    "            if word in skb_dict.keys() and docs_tuple[0][word] < threshold:\n",
    "                if len(skb_dict[word]) == 1:\n",
    "                    sentence_replace += list(skb_dict[word][0][1])\n",
    "                else:\n",
    "                    if word not in embedding_dict.keys():\n",
    "                        sentence_replace += list(skb_dict[word][0][1])\n",
    "                    else:\n",
    "                        senses_cos_list = []\n",
    "                        for (_, sememe_set) in skb_dict[word]:\n",
    "                            senses_cos_list.append(localAttention(\\\n",
    "                                                sentence, word, list(sememe_set), embedding_dict)[0])\n",
    "                            #print(senses_cos_list)\n",
    "                        if len(senses_cos_list) == 0:\n",
    "                            sentence_replace.append(word)\n",
    "                            continue\n",
    "                        senses_cos_list_max_index = senses_cos_list.index(max(senses_cos_list))\n",
    "                        sentence_replace += list(skb_dict[word][senses_cos_list_max_index][1])          \n",
    "            else:\n",
    "                sentence_replace.append(word)\n",
    "        sememe_docs_list.append(\" \".join(sentence_replace))\n",
    "    return sememe_docs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "47a520e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanBySKB(embedding_dict, skb_dict, doc_content_tuple, ferquncy_mix):\n",
    "    sememe_docs_list = replaceWord2Sememe(embedding_dict,\\\n",
    "                                      skb_dict,\\\n",
    "                                      doc_content_tuple,\\\n",
    "                                      ferquncy_mix)\n",
    "    clean_word_freq, clean_word_freq_sorted = evaluationFrequncy(sememe_docs_list,\"clran_ohsumed\",ferquncy_mix)\n",
    "    return (clean_word_freq, clean_word_freq_sorted,sememe_docs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6956cd",
   "metadata": {},
   "source": [
    "## Evluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513c4e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ohsumed_stc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5269300",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_content_tuple: tuple = np.load(\"Ohsumed_stc_list.npy\", allow_pickle=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0f9b53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8edbbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_content_tuple[2][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "90c40f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['behavior pulmonary circulation freedom responsibility activity work exercise miliary pathogenic patient impairment pathology',\n",
       " 'differential white blood cell import carry weight screening group streptococcal sepsis',\n",
       " 'invasive assessment cardiovascular eicosanoids thromboxane a2 platelet lipid vasodilator molecule manner random sampled males scope function future person formal potential influence attribute biological heredity genetics environment factors',\n",
       " 'increasing resistance parasitic bacteria Gram-positive infection silver density noun coin antibiotic gastrointestinal urine intestinal',\n",
       " 'issues cerebrospinal fluid management acid rapid surface speed stick insect common diagnosis spread blood stain culture']"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_content_tuple[2][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "e4332a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['behavior pulmonary circulation rest exercise miliary tuberculosis',\n",
       " 'differential white blood cell count screening group streptococcal sepsis',\n",
       " 'invasive assessment cardiovascular eicosanoids thromboxane a2 prostacyclin randomly sampled males special reference influence inheritance environmental factors',\n",
       " 'increasing resistance staphylococcus aureus ciprofloxacin',\n",
       " 'issues cerebrospinal fluid management acid fast bacillus smear culture']"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_content_tuple[2][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "eb56c99c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN',\n",
       "  {'energetic',\n",
       "   'gradual',\n",
       "   'impairment',\n",
       "   'laser',\n",
       "   'load',\n",
       "   'pathogenic',\n",
       "   'pathology',\n",
       "   'patient',\n",
       "   'rod-shaped',\n",
       "   'saprophytic'})]"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skb_da_pure[\"tuberculosis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "06035aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_content_tuple: tuple = np.load(\"../data/corpus/\" + dataset+\".clean.npy\", allow_pickle=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "fe9901e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 7400/7400 [00:46<00:00, 159.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======analysis start=======\n",
      "#all word size:  8466\n",
      "#frequncy < 10:  5644\n",
      "#frequncy mean:  15.265178360500826\n",
      "#frequncy standard deviation:  36.170276221630544\n",
      "#frequncy std/mean:  2.3694630594832993\n"
     ]
    }
   ],
   "source": [
    "doc_content_tuple = cleanBySKB(glove_840B_300d_common_crawl,skb_da_pure,doc_content_tuple,ferquncy_mix=10)\n",
    "np.save(\"cleandocs_stc_sg4_f10_once\",doc_content_tuple[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "36bb76da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 7400/7400 [00:44<00:00, 164.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======analysis start=======\n",
      "#all word size:  7834\n",
      "#frequncy < 10:  5763\n",
      "#frequncy mean:  12.21100331886648\n",
      "#frequncy standard deviation:  31.452145453076323\n",
      "#frequncy std/mean:  2.5757216366063487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "doc_content_tuple = cleanBySKB(glove_840B_300d_common_crawl,skb_da_upper_max,doc_content_tuple,ferquncy_mix=10)\n",
    "np.save(\"cleandocs_stc_sl4_f10_once\",doc_content_tuple[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "bf0d8ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 7400/7400 [01:03<00:00, 116.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======analysis start=======\n",
      "#all word size:  7714\n",
      "#frequncy < 23:  6518\n",
      "#frequncy mean:  14.942701581540057\n",
      "#frequncy standard deviation:  41.765352335444156\n",
      "#frequncy std/mean:  2.795033555849119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "doc_content_tuple = cleanBySKB(glove_840B_300d_common_crawl,skb_da_upper_max,doc_content_tuple,ferquncy_mix=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665135c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "9a43f656",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"cleandocs_stc_sl4_f23_n\",doc_content_tuple[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "2dfadb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 7400/7400 [01:38<00:00, 74.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======analysis start=======\n",
      "#all word size:  8022\n",
      "#frequncy < 23:  6304\n",
      "#frequncy mean:  22.80491149339317\n",
      "#frequncy standard deviation:  60.5398958834466\n",
      "#frequncy std/mean:  2.6546867283824214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "doc_content_tuple = cleanBySKB(glove_840B_300d_common_crawl,skb_da_pure,doc_content_tuple,ferquncy_mix=23)\n",
    "np.save(\"cleandocs_stc_sg4_f23_n\",doc_content_tuple[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c48a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_content_tuple: tuple = np.load(\"../data/corpus/\" + dataset+\".clean.npy\", allow_pickle=True).tolist()\n",
    "for i in range(5):\n",
    "    doc_content_tuple = cleanBySKB(glove_840B_300d_common_crawl,skb_da_pure,doc_content_tuple,ferquncy_mix=5)\n",
    "np.save(\"cleandocs_sg4_f5\",doc_content_tuple_sg4_f5[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07df045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348e4e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_content_tuple: tuple = np.load(\"../data/corpus/\" + dataset+\".clean.npy\", allow_pickle=True).tolist()\n",
    "for i in range(5):\n",
    "    doc_content_tuple = cleanBySKB(glove_840B_300d_common_crawl,skb_da_upper_max,doc_content_tuple,ferquncy_mix=5)\n",
    "np.save(\"cleandocs_sl4_f5\",doc_content_tuple[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "080b319f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "3c031a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 7400/7400 [03:24<00:00, 36.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======analysis start=======\n",
      "#all word size:  24305\n",
      "#frequncy < 10:  14300\n",
      "#frequncy mean:  35.33918946718782\n",
      "#frequncy standard deviation:  125.2461363915609\n",
      "#frequncy std/mean:  3.5441145730818477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 7400/7400 [01:03<00:00, 116.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======analysis start=======\n",
      "#all word size:  23836\n",
      "#frequncy < 10:  13796\n",
      "#frequncy mean:  36.7106897130391\n",
      "#frequncy standard deviation:  127.83564104285078\n",
      "#frequncy std/mean:  3.4822456903457586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 7400/7400 [00:22<00:00, 325.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======analysis start=======\n",
      "#all word size:  23623\n",
      "#frequncy < 10:  13580\n",
      "#frequncy mean:  37.251703847944796\n",
      "#frequncy standard deviation:  128.8301377500898\n",
      "#frequncy std/mean:  3.4583689990651916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 7400/7400 [00:08<00:00, 907.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======analysis start=======\n",
      "#all word size:  23554\n",
      "#frequncy < 10:  13509\n",
      "#frequncy mean:  37.42684894285472\n",
      "#frequncy standard deviation:  129.14875224470933\n",
      "#frequncy std/mean:  3.45069798533936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 7400/7400 [00:03<00:00, 1906.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======analysis start=======\n",
      "#all word size:  23528\n",
      "#frequncy < 10:  13483\n",
      "#frequncy mean:  37.505610336620194\n",
      "#frequncy standard deviation:  129.28755344121296\n",
      "#frequncy std/mean:  3.4471523668281057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 7400/7400 [00:02<00:00, 2955.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======analysis start=======\n",
      "#all word size:  23502\n",
      "#frequncy < 10:  13456\n",
      "#frequncy mean:  37.56705812271296\n",
      "#frequncy standard deviation:  129.39244790439454\n",
      "#frequncy std/mean:  3.4443061120658838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 7400/7400 [00:01<00:00, 5010.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======analysis start=======\n",
      "#all word size:  23499\n",
      "#frequncy < 10:  13453\n",
      "#frequncy mean:  37.58274820205115\n",
      "#frequncy standard deviation:  129.41764468685346\n",
      "#frequncy std/mean:  3.4435386148741047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 7400/7400 [00:01<00:00, 6386.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======analysis start=======\n",
      "#all word size:  23496\n",
      "#frequncy < 10:  13449\n",
      "#frequncy mean:  37.59776132107593\n",
      "#frequncy standard deviation:  129.43635756862727\n",
      "#frequncy std/mean:  3.4426612920719295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 7400/7400 [00:01<00:00, 6020.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======analysis start=======\n",
      "#all word size:  23495\n",
      "#frequncy < 10:  13448\n",
      "#frequncy mean:  37.60825707597361\n",
      "#frequncy standard deviation:  129.45151969849783\n",
      "#frequncy std/mean:  3.4421036698666674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 7400/7400 [00:01<00:00, 6528.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======analysis start=======\n",
      "#all word size:  23492\n",
      "#frequncy < 10:  13445\n",
      "#frequncy mean:  37.62216924910608\n",
      "#frequncy standard deviation:  129.46935707276438\n",
      "#frequncy std/mean:  3.4413049448455353\n"
     ]
    }
   ],
   "source": [
    "doc_content_tuple: tuple = np.load(\"../data/corpus/\" + dataset+\".clean.npy\", allow_pickle=True).tolist()\n",
    "for i in range(10):\n",
    "    doc_content_tuple = cleanBySKB(glove_840B_300d_common_crawl,skb_da_pure,doc_content_tuple,ferquncy_mix=10)\n",
    "np.save(\"cleandocs_sg4_f10\",doc_content_tuple[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef007bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "061fd113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 7400/7400 [03:19<00:00, 37.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======analysis start=======\n",
      "#all word size:  23947\n",
      "#frequncy < 5:  14585\n",
      "#frequncy mean:  33.03942038668727\n",
      "#frequncy standard deviation:  120.3385337652907\n",
      "#frequncy std/mean:  3.6422713339662365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 7400/7400 [01:12<00:00, 101.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======analysis start=======\n",
      "#all word size:  23305\n",
      "#frequncy < 5:  13908\n",
      "#frequncy mean:  34.42342844883072\n",
      "#frequncy standard deviation:  123.13422827310134\n",
      "#frequncy std/mean:  3.577047197844813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 7400/7400 [00:23<00:00, 311.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======analysis start=======\n",
      "#all word size:  23021\n",
      "#frequncy < 5:  13622\n",
      "#frequncy mean:  34.99192042048564\n",
      "#frequncy standard deviation:  124.23272748722385\n",
      "#frequncy std/mean:  3.5503260751157044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 7400/7400 [00:09<00:00, 813.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======analysis start=======\n",
      "#all word size:  22899\n",
      "#frequncy < 5:  13499\n",
      "#frequncy mean:  35.2279138827023\n",
      "#frequncy standard deviation:  124.67057066834246\n",
      "#frequncy std/mean:  3.53897114326598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 7400/7400 [00:02<00:00, 2767.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======analysis start=======\n",
      "#all word size:  22869\n",
      "#frequncy < 5:  13469\n",
      "#frequncy mean:  35.29192356465084\n",
      "#frequncy standard deviation:  124.78245762386666\n",
      "#frequncy std/mean:  3.5357227665780027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 7400/7400 [00:02<00:00, 3175.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======analysis start=======\n",
      "#all word size:  22856\n",
      "#frequncy < 5:  13454\n",
      "#frequncy mean:  35.323722436121805\n",
      "#frequncy standard deviation:  124.82886706438053\n",
      "#frequncy std/mean:  3.533853695349258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 7400/7400 [00:01<00:00, 5951.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======analysis start=======\n",
      "#all word size:  22845\n",
      "#frequncy < 5:  13443\n",
      "#frequncy mean:  35.349135478222806\n",
      "#frequncy standard deviation:  124.86868835804128\n",
      "#frequncy std/mean:  3.532439667017257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 7400/7400 [00:00<00:00, 7986.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======analysis start=======\n",
      "#all word size:  22841\n",
      "#frequncy < 5:  13439\n",
      "#frequncy mean:  35.36053587846416\n",
      "#frequncy standard deviation:  124.88415892125731\n",
      "#frequncy std/mean:  3.531738301435535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 7400/7400 [00:00<00:00, 11505.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======analysis start=======\n",
      "#all word size:  22839\n",
      "#frequncy < 5:  13437\n",
      "#frequncy mean:  35.36805464337318\n",
      "#frequncy standard deviation:  124.89476881170789\n",
      "#frequncy std/mean:  3.531287487283644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 7400/7400 [00:00<00:00, 9665.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======analysis start=======\n",
      "#all word size:  22840\n",
      "#frequncy < 5:  13438\n",
      "#frequncy mean:  35.37105954465849\n",
      "#frequncy standard deviation:  124.8963314895312\n",
      "#frequncy std/mean:  3.531031671014566\n"
     ]
    }
   ],
   "source": [
    "doc_content_tuple: tuple = np.load(\"../data/corpus/\" + dataset+\".clean.npy\", allow_pickle=True).tolist()\n",
    "for i in range(10):\n",
    "    doc_content_tuple = cleanBySKB(glove_840B_300d_common_crawl,skb_da_upper_max,doc_content_tuple,ferquncy_mix=10)\n",
    "np.save(\"cleandocs_sl4_f10\",doc_content_tuple[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "9fc516ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 7400/7400 [03:13<00:00, 38.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======analysis start=======\n",
      "#all word size:  24305\n",
      "#frequncy < 5:  11634\n",
      "#frequncy mean:  35.33918946718782\n",
      "#frequncy standard deviation:  125.2461363915609\n",
      "#frequncy std/mean:  3.5441145730818477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "doc_content_tuple = cleanBySKB(glove_840B_300d_common_crawl,skb_da_pure,doc_content_tuple,ferquncy_mix=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "1b04e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"cleandocs_sg4_f10_1\",doc_content_tuple[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "2820f50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 7400/7400 [00:22<00:00, 325.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======analysis start=======\n",
      "#all word size:  28843\n",
      "#frequncy < 5:  15412\n",
      "#frequncy mean:  25.22487258606941\n",
      "#frequncy standard deviation:  102.22815100329845\n",
      "#frequncy std/mean:  4.052672641040596\n"
     ]
    }
   ],
   "source": [
    "doc_content_tuple: tuple = np.load(\"../data/corpus/\" + dataset+\".clean.npy\", allow_pickle=True).tolist()\n",
    "doc_content_tuple = cleanBySKB(glove_840B_300d_common_crawl,dictskb,doc_content_tuple,ferquncy_mix=5)\n",
    "np.save(\"cleandocs_dictskb_sl4_f10\",doc_content_tuple[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300f50ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
